{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdfdafc",
   "metadata": {},
   "source": [
    "# 情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bfe23",
   "metadata": {},
   "source": [
    "### 1. 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925b5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import os\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoConfig, AutoTokenizer, BertForSequenceClassification\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b73b97",
   "metadata": {},
   "source": [
    "### 2. 设置超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b41d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda\n"
     ]
    }
   ],
   "source": [
    "max_length = 512                                                     # 最大长度\n",
    "batch_size = 20                                                      # 批量大小\n",
    "learning_rate = 1e-5                                                 # 学习率\n",
    "epoch_num = 2                                                        # epoch数\n",
    "themes = {\"动力\", \"价格\", \"内饰\", \"配置\", \"安全性\", \"外观\", \"操控\", \"油耗\", \"空间\", \"舒适性\"}\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'              # 设备(cuda)\n",
    "print(f'using device {device}')\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"                                     # 预训练模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)                # 分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d706cd",
   "metadata": {},
   "source": [
    "### 3. 设置随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60e1b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(12)                                                  # 随机种子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d812341",
   "metadata": {},
   "source": [
    "### 4. 定义Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218fdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChnSentiCorp(Dataset):\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.themes = [\"动力\", \"价格\", \"内饰\", \"配置\", \"安全性\", \"外观\", \"操控\", \"油耗\", \"空间\", \"舒适性\"]\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        theme_sentiment_pattern = re.compile(r'(\\S+?)#(-?\\d+)')\n",
    "        Data = {}\n",
    "\n",
    "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                line = line.strip()\n",
    "                matches = theme_sentiment_pattern.findall(line)\n",
    "                comment = re.sub(theme_sentiment_pattern, \"\", line).strip()\n",
    "                theme_in_line = [theme for theme, _ in matches]\n",
    "                multi_hot_vector = [1 if theme in theme_in_line else 0 for theme in self.themes]\n",
    "                total_sentiment = sum(int(senti) for _, senti in matches)\n",
    "\n",
    "                if total_sentiment > 0:\n",
    "                    sentiment_label = 2\n",
    "                elif total_sentiment < 0:\n",
    "                    sentiment_label = 0\n",
    "                else:\n",
    "                    sentiment_label = 1\n",
    "\n",
    "                Data[idx] = {\n",
    "                    \"comment\": comment.replace(\" \", \"\"),\n",
    "                    \"themes\": multi_hot_vector,\n",
    "                    \"sentiment\": sentiment_label\n",
    "                }\n",
    "            return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1f841e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set: 8000\n",
      "length of test set: 2653\n",
      "{'comment': '因为森林人即将换代，这套系统没必要装在一款即将换代的车型上，因为肯定会影响价格。', 'themes': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'sentiment': 1}\n"
     ]
    }
   ],
   "source": [
    "train_data = ChnSentiCorp('data/train.txt')\n",
    "test_data = ChnSentiCorp('data/test.txt')\n",
    "\n",
    "print(f\"length of training set: {len(train_data)}\")\n",
    "print(f\"length of test set: {len(test_data)}\")\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecf7e67",
   "metadata": {},
   "source": [
    "### 5. 定义DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add971d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_samples):\n",
    "    batch_sentences, batch_sentiment_labels = [], []\n",
    "\n",
    "    for sample in batch_samples:\n",
    "        batch_sentences.append(sample['comment'])\n",
    "        batch_sentiment_labels.append(sample['sentiment'])\n",
    "\n",
    "    batch_inputs = tokenizer(\n",
    "        batch_sentences,\n",
    "        max_length=max_length,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    input_ids = batch_inputs['input_ids']\n",
    "    attention_mask = batch_inputs['attention_mask']\n",
    "    labels = torch.tensor(batch_sentiment_labels, dtype=torch.long)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids, \n",
    "        'attention_mask': attention_mask,  \n",
    "        'labels': labels  \n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d689b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1094, 6887,  ...,    0,    0,    0],\n",
       "         [ 101, 4684, 2970,  ...,    0,    0,    0],\n",
       "         [ 101, 2769, 4500,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101,  100, 4696,  ...,    0,    0,    0],\n",
       "         [ 101, 1963, 3362,  ...,    0,    0,    0],\n",
       "         [ 101, 2990, 6756,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841e37f",
   "metadata": {},
   "source": [
    "### 6. 配置模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20d9a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForSequenceClassification.from_pretrained(checkpoint, num_labels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1448150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-chinese\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1621a7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3e000d",
   "metadata": {},
   "source": [
    "### 7. 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "641c9131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\nlp_cw1\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num * len(train_dataloader),\n",
    ")\n",
    "\n",
    "writer = SummaryWriter(log_dir='sentiment_classification_logs' + '/' + time.strftime('%m-%d_%H.%M', time.localtime()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0cf47d",
   "metadata": {},
   "source": [
    "### 8. 定义train函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2f933ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_step = 0\n",
    "total_train_loss = 0.\n",
    "best_f1_score = 0.\n",
    "total_test_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "309120e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_train_loss, total_train_step):\n",
    "    progress_bar = tqdm(range(len(dataloader)),disable=False)\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_step_num = epoch * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for step, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = {k: torch.tensor(v).to(device) if isinstance(v, list) else v.to(device) for k, v in batch_data.items()}\n",
    "        theme_outputs = model(**batch_data)\n",
    "        loss = theme_outputs.loss\n",
    "        logits = theme_outputs.logits\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_train_step +=1\n",
    "        progress_bar.set_description(f'loss: {total_train_loss / (finish_step_num + step):>7f}')\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        if total_train_step % 100 == 0:\n",
    "            print(\"训练次数:{}，loss:{}\".format(total_train_step, loss.item()))\n",
    "            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n",
    "\n",
    "    return total_train_loss, total_train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6396515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, epoch):\n",
    "    true_labels, predictions = [], []\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for step, batch_data in enumerate(dataloader, start=1):\n",
    "            batch_data = {k: torch.tensor(v).to(device) if isinstance(v, list) else v.to(device) for k, v in batch_data.items()}\n",
    "            theme_outputs = model(**batch_data)\n",
    "            loss = theme_outputs.loss\n",
    "            logits = theme_outputs.logits\n",
    "\n",
    "            pred = logits.argmax(dim=-1)\n",
    "\n",
    "            true_labels += batch_data[\"labels\"].cpu().numpy().tolist()\n",
    "            predictions += pred.cpu().numpy().tolist()\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "    print(\"整体测试集上的Loss:{}\".format(total_test_loss))\n",
    "    writer.add_scalar(\"test_loss\", total_test_loss, epoch)\n",
    "\n",
    "    metrics = classification_report(true_labels, predictions, output_dict=True)\n",
    "\n",
    "    pos_p, pos_r, pos_f1 = metrics['2']['precision'], metrics['2']['recall'], metrics['2']['f1-score']  # 正向情感 (类别 2)\n",
    "\n",
    "    neu_p, neu_r, neu_f1= metrics['1']['precision'], metrics['1']['recall'], metrics['1']['f1-score']   # 中性情感 (类别 1)\n",
    "\n",
    "    neg_p, neg_r, neg_f1 = metrics['0']['precision'], metrics['0']['recall'], metrics['0']['f1-score'] # 负向情感 (类别 0)\n",
    "\n",
    "    macro_f1 = metrics['macro avg']['f1-score']\n",
    "    micro_f1 = metrics['weighted avg']['f1-score']\n",
    "    accuracy = metrics['accuracy']\n",
    "    writer.add_scalar(\"test_accuarcy\", accuracy, epoch)\n",
    "\n",
    "    print(f\"Positive (2): Precision: {pos_p * 100:>0.2f} / Recall: {pos_r * 100:>0.2f} / F1: {pos_f1 * 100:>0.2f}\")\n",
    "    print(f\"Neutral (1): Precision: {neu_p * 100:>0.2f} / Recall: {neu_r * 100:>0.2f} / F1: {neu_f1 * 100:>0.2f}\")\n",
    "    print(f\"Negative (0): Precision: {neg_p * 100:>0.2f} / Recall: {neg_r * 100:>0.2f} / F1: {neg_f1 * 100:>0.2f}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:>0.2f}\")\n",
    "    print(f\"Macro-F1: {macro_f1 * 100:>0.2f} / Micro-F1: {micro_f1 * 100:>0.2f}\\n\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae027c50",
   "metadata": {},
   "source": [
    "### 9. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cc804c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1475948742e4c61818c8689907a6803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数:100，loss:0.8188006281852722\n",
      "训练次数:200，loss:0.49138039350509644\n",
      "训练次数:300，loss:0.4329761564731598\n",
      "训练次数:400，loss:0.7496187090873718\n",
      "整体测试集上的Loss:87.31010556221008\n",
      "Positive (2): Precision: 58.05 / Recall: 31.56 / F1: 40.89\n",
      "Neutral (1): Precision: 74.02 / Recall: 93.03 / F1: 82.44\n",
      "Negative (0): Precision: 62.03 / Recall: 21.59 / F1: 32.03\n",
      "Accuracy: 72.07\n",
      "Macro-F1: 51.79 / Micro-F1: 67.91\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/2\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce8ce5bae28411cb7fca1cb21ca124b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练次数:500，loss:0.15900513529777527\n",
      "训练次数:600，loss:0.40734371542930603\n",
      "训练次数:700，loss:0.2783155143260956\n",
      "训练次数:800，loss:0.28716492652893066\n",
      "整体测试集上的Loss:88.30534793436527\n",
      "Positive (2): Precision: 53.65 / Recall: 44.83 / F1: 48.84\n",
      "Neutral (1): Precision: 77.31 / Recall: 86.22 / F1: 81.53\n",
      "Negative (0): Precision: 53.92 / Recall: 36.34 / F1: 43.42\n",
      "Accuracy: 71.81\n",
      "Macro-F1: 57.93 / Micro-F1: 70.36\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    print(f\"Epoch {epoch + 1}/{epoch_num}\\n\" + 30 * \"-\")\n",
    "    total_train_loss, total_train_step= train_loop(train_dataloader, model, optimizer, lr_scheduler, epoch, total_train_loss, total_train_step)\n",
    "    valid_scores = test_loop(test_dataloader, model, epoch)\n",
    "    macro_f1, micro_f1 = valid_scores['macro avg']['f1-score'], valid_scores['weighted avg']['f1-score']\n",
    "    f1_score = (macro_f1 + micro_f1) / 2\n",
    "    if f1_score > best_f1_score:\n",
    "        best_f1_score = f1_score\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f'epoch_{epoch + 1}_valid_macrof1_{(macro_f1 * 100):0.3f}_microf1_{(micro_f1 * 100):0.3f}_model_weights.bin'\n",
    "        )\n",
    "\n",
    "writer.close()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef06c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_cw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
